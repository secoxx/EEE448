{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP-IZxAEZ4is"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VCASiSxYW4Zh",
        "outputId": "43b7e23d-cb5f-42c3-f97a-8c3cd6180584"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 73/73 [00:00<00:00, 1294.66it/s]\n",
            "100%|██████████| 5000/5000 [17:40<00:00,  4.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current state is [[0, 0], [2, 0]]\n",
            "current state is [[0, 0], [2, 1]]\n",
            "current state is [[0, 1], [1, 1]]\n",
            "current state is [[1, 1], [1, 2]]\n",
            "current state is [[1, 2], [1, 2]]\n",
            "Player 0 follows the  policy : stay-up-right-up of length 4.\n",
            "Player 1 follows the  policy : up-left-up-stay of length 4.\n"
          ]
        }
      ],
      "source": [
        "from NashQLearn import Player, Grid\n",
        "#Initialize the two players\n",
        "player1 = Player([0,0])\n",
        "player2 = Player([2,0])\n",
        "#Initialize the grid\n",
        "grid = Grid(length = 3,\n",
        "            width = 3,\n",
        "            players = [player1,player2],\n",
        "            obstacle_coordinates = [],\n",
        "            reward_coordinates = [1,2],\n",
        "            reward_value = 100,\n",
        "            collision_penalty = -1)\n",
        "from NashQLearn import NashQLearning\n",
        "nashQ = NashQLearning(grid,\n",
        "                      max_iter = 100,\n",
        "                      discount_factor = 0.7,\n",
        "                      learning_rate = 0.7,\n",
        "                      epsilon = 0.5,\n",
        "                      decision_strategy = 'random') #Available strategies : 'random', 'greedy', and 'epsilon-greedy'\n",
        "#Retrieve the Q tables after fitting the algorithm\n",
        "Q0, Q1 = nashQ.fit(return_history = False)\n",
        "#Best path followed by each player given the values in the Q tables\n",
        "p0, p1 = nashQ.get_best_policy(Q0,Q1)\n",
        "\n",
        "#Show the results\n",
        "print('Player 0 follows the  policy : %s of length %s.'%('-'.join(p0),len(p0)))\n",
        "print('Player 1 follows the  policy : %s of length %s.'%('-'.join(p1),len(p1)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}